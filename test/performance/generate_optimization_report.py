#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–æ•ˆæœåˆ†ææŠ¥å‘Šç”Ÿæˆå™¨
æ”¶é›†A/Bæµ‹è¯•æ•°æ®ï¼Œç”Ÿæˆç»¼åˆæ€§èƒ½ä¼˜åŒ–åˆ†ææŠ¥å‘Š
æä¾›å†³ç­–å»ºè®®å’Œåç»­ä¼˜åŒ–æ–¹å‘
"""

import json
import os
import glob
import statistics
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import numpy as np
from dataclasses import dataclass

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

@dataclass
class OptimizationSummary:
    """ä¼˜åŒ–æ€»ç»“"""
    baseline_performance: Dict[str, float]
    optimized_performance: Dict[str, float]
    improvement_metrics: Dict[str, float]
    recommendation: str
    confidence_level: str

class OptimizationReportGenerator:
    def __init__(self):
        self.reports_dir = "test/performance/reports"
        self.output_dir = f"{self.reports_dir}/final_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        self.baseline_data = None
        self.ab_test_data = None
        self.tuning_data = None
        
        # æ€§èƒ½è¯„ä¼°æ ‡å‡†
        self.performance_standards = {
            'excellent': {'response_time': 200, 'success_rate': 99.5, 'improvement': 20},
            'good': {'response_time': 500, 'success_rate': 99.0, 'improvement': 10},
            'acceptable': {'response_time': 1000, 'success_rate': 95.0, 'improvement': 5},
            'poor': {'response_time': 2000, 'success_rate': 90.0, 'improvement': 0}
        }
    
    def collect_test_data(self) -> bool:
        """æ”¶é›†æ‰€æœ‰æµ‹è¯•æ•°æ®"""
        print("æ”¶é›†æµ‹è¯•æ•°æ®...")
        
        try:
            # æ”¶é›†åŸºçº¿æµ‹è¯•æ•°æ®
            baseline_files = glob.glob(f"{self.reports_dir}/baseline_*/*.json")
            if baseline_files:
                latest_baseline = max(baseline_files, key=os.path.getctime)
                with open(latest_baseline, 'r', encoding='utf-8') as f:
                    self.baseline_data = json.load(f)
                print(f"âœ… åŸºçº¿æ•°æ®: {latest_baseline}")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°åŸºçº¿æµ‹è¯•æ•°æ®")
            
            # æ”¶é›†A/Bæµ‹è¯•æ•°æ®
            ab_test_files = glob.glob(f"{self.reports_dir}/ab_test_*/ab_analysis.json")
            if ab_test_files:
                latest_ab_test = max(ab_test_files, key=os.path.getctime)
                with open(latest_ab_test, 'r', encoding='utf-8') as f:
                    self.ab_test_data = json.load(f)
                print(f"âœ… A/Bæµ‹è¯•æ•°æ®: {latest_ab_test}")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°A/Bæµ‹è¯•æ•°æ®")
            
            # æ”¶é›†è°ƒä¼˜æ•°æ®
            tuning_files = glob.glob(f"{self.reports_dir}/tuning_report_*.json")
            if tuning_files:
                latest_tuning = max(tuning_files, key=os.path.getctime)
                with open(latest_tuning, 'r', encoding='utf-8') as f:
                    self.tuning_data = json.load(f)
                print(f"âœ… è°ƒä¼˜æ•°æ®: {latest_tuning}")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°è°ƒä¼˜æ•°æ®")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ”¶é›†æ•°æ®å¤±è´¥: {e}")
            return False
    
    def analyze_baseline_performance(self) -> Dict[str, Any]:
        """åˆ†æåŸºçº¿æ€§èƒ½"""
        if not self.baseline_data:
            return {}
        
        analysis = {
            'summary': {},
            'bottlenecks': [],
            'performance_grade': 'unknown'
        }
        
        try:
            # åˆ†æå„æ“ä½œç±»å‹çš„æ€§èƒ½
            operations_summary = {}
            
            for test_name, test_data in self.baseline_data.items():
                if isinstance(test_data, dict) and 'operations' in test_data:
                    for op_type, metrics in test_data['operations'].items():
                        if op_type not in operations_summary:
                            operations_summary[op_type] = []
                        operations_summary[op_type].append(metrics)
            
            # è®¡ç®—ç»¼åˆæŒ‡æ ‡
            for op_type, metrics_list in operations_summary.items():
                if metrics_list:
                    avg_metrics = {
                        'avg_response_time': statistics.mean([m.get('avg_response_time', 0) for m in metrics_list]),
                        'success_rate': statistics.mean([m.get('success_rate', 0) for m in metrics_list]),
                        'p95_response_time': statistics.mean([m.get('p95_response_time', 0) for m in metrics_list]),
                        'throughput': statistics.mean([m.get('throughput', 0) for m in metrics_list])
                    }
                    
                    analysis['summary'][op_type] = avg_metrics
                    
                    # è¯†åˆ«ç“¶é¢ˆ
                    if avg_metrics['avg_response_time'] > 1000:
                        analysis['bottlenecks'].append({
                            'operation': op_type,
                            'issue': 'å“åº”æ—¶é—´è¿‡é•¿',
                            'value': avg_metrics['avg_response_time'],
                            'severity': 'high'
                        })
                    
                    if avg_metrics['success_rate'] < 95:
                        analysis['bottlenecks'].append({
                            'operation': op_type,
                            'issue': 'æˆåŠŸç‡åä½',
                            'value': avg_metrics['success_rate'],
                            'severity': 'high'
                        })
            
            # è¯„ä¼°æ•´ä½“æ€§èƒ½ç­‰çº§
            if operations_summary:
                avg_response_time = statistics.mean([
                    metrics['avg_response_time'] 
                    for metrics in analysis['summary'].values()
                ])
                avg_success_rate = statistics.mean([
                    metrics['success_rate'] 
                    for metrics in analysis['summary'].values()
                ])
                
                if avg_response_time <= 200 and avg_success_rate >= 99.5:
                    analysis['performance_grade'] = 'excellent'
                elif avg_response_time <= 500 and avg_success_rate >= 99.0:
                    analysis['performance_grade'] = 'good'
                elif avg_response_time <= 1000 and avg_success_rate >= 95.0:
                    analysis['performance_grade'] = 'acceptable'
                else:
                    analysis['performance_grade'] = 'poor'
            
        except Exception as e:
            print(f"åˆ†æåŸºçº¿æ€§èƒ½å¤±è´¥: {e}")
        
        return analysis
    
    def analyze_optimization_effect(self) -> Dict[str, Any]:
        """åˆ†æä¼˜åŒ–æ•ˆæœ"""
        if not self.ab_test_data:
            return {}
        
        analysis = {
            'improvements': {},
            'overall_effect': 'unknown',
            'significant_improvements': [],
            'areas_for_improvement': []
        }
        
        try:
            if 'comparison' in self.ab_test_data:
                comparison = self.ab_test_data['comparison']
                
                total_improvements = []
                
                for op_type, comp_data in comparison.items():
                    improvement = {
                        'operation': op_type,
                        'response_time_improvement': comp_data.get('response_time_improvement', 0),
                        'success_rate_improvement': comp_data.get('success_rate_improvement', 0),
                        'p95_improvement': comp_data.get('p95_improvement', 0)
                    }
                    
                    analysis['improvements'][op_type] = improvement
                    total_improvements.append(improvement['response_time_improvement'])
                    
                    # è¯†åˆ«æ˜¾è‘—æ”¹è¿›
                    if improvement['response_time_improvement'] > 15:
                        analysis['significant_improvements'].append({
                            'operation': op_type,
                            'improvement': improvement['response_time_improvement'],
                            'type': 'å“åº”æ—¶é—´æ˜¾è‘—æ”¹è¿›'
                        })
                    
                    # è¯†åˆ«éœ€è¦æ”¹è¿›çš„é¢†åŸŸ
                    if improvement['response_time_improvement'] < 5:
                        analysis['areas_for_improvement'].append({
                            'operation': op_type,
                            'improvement': improvement['response_time_improvement'],
                            'suggestion': 'è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–æˆ–é‡‡ç”¨å…¶ä»–ç­–ç•¥'
                        })
                
                # è¯„ä¼°æ•´ä½“æ•ˆæœ
                if total_improvements:
                    avg_improvement = statistics.mean(total_improvements)
                    
                    if avg_improvement > 20:
                        analysis['overall_effect'] = 'excellent'
                    elif avg_improvement > 10:
                        analysis['overall_effect'] = 'good'
                    elif avg_improvement > 5:
                        analysis['overall_effect'] = 'acceptable'
                    else:
                        analysis['overall_effect'] = 'poor'
        
        except Exception as e:
            print(f"åˆ†æä¼˜åŒ–æ•ˆæœå¤±è´¥: {e}")
        
        return analysis
    
    def generate_comprehensive_charts(self, baseline_analysis: Dict, optimization_analysis: Dict):
        """ç”Ÿæˆç»¼åˆå›¾è¡¨"""
        try:
            # 1. æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾
            self.create_performance_radar_chart(baseline_analysis, optimization_analysis)
            
            # 2. æ”¹è¿›æ•ˆæœæŸ±çŠ¶å›¾
            self.create_improvement_bar_chart(optimization_analysis)
            
            # 3. æ€§èƒ½è¶‹åŠ¿å›¾
            self.create_performance_trend_chart()
            
            # 4. ç“¶é¢ˆåˆ†æå›¾
            self.create_bottleneck_analysis_chart(baseline_analysis)
            
        except Exception as e:
            print(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")
    
    def create_performance_radar_chart(self, baseline_analysis: Dict, optimization_analysis: Dict):
        """åˆ›å»ºæ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾"""
        if not baseline_analysis.get('summary') or not optimization_analysis.get('improvements'):
            return
        
        # å‡†å¤‡æ•°æ®
        operations = list(baseline_analysis['summary'].keys())
        if not operations:
            return
        
        # è®¡ç®—å„ç»´åº¦å¾—åˆ†ï¼ˆ0-100ï¼‰
        baseline_scores = []
        optimized_scores = []
        
        for op in operations:
            baseline_metrics = baseline_analysis['summary'][op]
            
            # åŸºçº¿å¾—åˆ†ï¼ˆå“åº”æ—¶é—´è¶Šä½è¶Šå¥½ï¼ŒæˆåŠŸç‡è¶Šé«˜è¶Šå¥½ï¼‰
            response_score = max(0, 100 - (baseline_metrics['avg_response_time'] / 10))
            success_score = baseline_metrics['success_rate']
            baseline_score = (response_score + success_score) / 2
            baseline_scores.append(baseline_score)
            
            # ä¼˜åŒ–åå¾—åˆ†
            if op in optimization_analysis['improvements']:
                improvement = optimization_analysis['improvements'][op]['response_time_improvement']
                optimized_score = baseline_score * (1 + improvement / 100)
            else:
                optimized_score = baseline_score
            
            optimized_scores.append(min(100, optimized_score))
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(operations), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢
        
        baseline_scores += baseline_scores[:1]
        optimized_scores += optimized_scores[:1]
        
        fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
        
        ax.plot(angles, baseline_scores, 'o-', linewidth=2, label='ä¼˜åŒ–å‰', color='red')
        ax.fill(angles, baseline_scores, alpha=0.25, color='red')
        
        ax.plot(angles, optimized_scores, 'o-', linewidth=2, label='ä¼˜åŒ–å', color='green')
        ax.fill(angles, optimized_scores, alpha=0.25, color='green')
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(operations)
        ax.set_ylim(0, 100)
        ax.set_title('æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾', size=16, pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/performance_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_improvement_bar_chart(self, optimization_analysis: Dict):
        """åˆ›å»ºæ”¹è¿›æ•ˆæœæŸ±çŠ¶å›¾"""
        if not optimization_analysis.get('improvements'):
            return
        
        operations = list(optimization_analysis['improvements'].keys())
        improvements = [optimization_analysis['improvements'][op]['response_time_improvement'] 
                      for op in operations]
        
        plt.figure(figsize=(12, 6))
        colors = ['green' if x > 0 else 'red' for x in improvements]
        bars = plt.bar(operations, improvements, color=colors, alpha=0.7)
        
        plt.title('RocketMQä¼˜åŒ–æ•ˆæœ - å“åº”æ—¶é—´æ”¹è¿›', size=16)
        plt.ylabel('æ”¹è¿›ç™¾åˆ†æ¯” (%)')
        plt.xlabel('æ“ä½œç±»å‹')
        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, improvements):
            plt.text(bar.get_x() + bar.get_width()/2, 
                    bar.get_height() + (1 if value > 0 else -3),
                    f'{value:.1f}%', ha='center', 
                    va='bottom' if value > 0 else 'top')
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/improvement_bar_chart.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_performance_trend_chart(self):
        """åˆ›å»ºæ€§èƒ½è¶‹åŠ¿å›¾"""
        if not self.tuning_data or 'performance_history' not in self.tuning_data:
            return
        
        history = self.tuning_data['performance_history']
        if not history:
            return
        
        timestamps = [datetime.fromisoformat(h['timestamp']) for h in history]
        response_times = [h['avg_response_time'] for h in history]
        success_rates = [h['success_rate'] for h in history]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        # å“åº”æ—¶é—´è¶‹åŠ¿
        ax1.plot(timestamps, response_times, 'b-', linewidth=2, marker='o')
        ax1.set_title('å“åº”æ—¶é—´è¶‹åŠ¿', size=14)
        ax1.set_ylabel('å“åº”æ—¶é—´ (ms)')
        ax1.grid(True, alpha=0.3)
        
        # æˆåŠŸç‡è¶‹åŠ¿
        ax2.plot(timestamps, success_rates, 'g-', linewidth=2, marker='s')
        ax2.set_title('æˆåŠŸç‡è¶‹åŠ¿', size=14)
        ax2.set_ylabel('æˆåŠŸç‡ (%)')
        ax2.set_xlabel('æ—¶é—´')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/performance_trend.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_bottleneck_analysis_chart(self, baseline_analysis: Dict):
        """åˆ›å»ºç“¶é¢ˆåˆ†æå›¾"""
        if not baseline_analysis.get('bottlenecks'):
            return
        
        bottlenecks = baseline_analysis['bottlenecks']
        operations = [b['operation'] for b in bottlenecks]
        values = [b['value'] for b in bottlenecks]
        issues = [b['issue'] for b in bottlenecks]
        
        plt.figure(figsize=(10, 6))
        colors = ['red' if 'å“åº”æ—¶é—´' in issue else 'orange' for issue in issues]
        bars = plt.bar(operations, values, color=colors, alpha=0.7)
        
        plt.title('æ€§èƒ½ç“¶é¢ˆåˆ†æ', size=16)
        plt.ylabel('æŒ‡æ ‡å€¼')
        plt.xlabel('æ“ä½œç±»å‹')
        
        # æ·»åŠ å›¾ä¾‹
        import matplotlib.patches as mpatches
        red_patch = mpatches.Patch(color='red', alpha=0.7, label='å“åº”æ—¶é—´é—®é¢˜')
        orange_patch = mpatches.Patch(color='orange', alpha=0.7, label='æˆåŠŸç‡é—®é¢˜')
        plt.legend(handles=[red_patch, orange_patch])
        
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/bottleneck_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_optimization_summary(self, baseline_analysis: Dict, optimization_analysis: Dict) -> OptimizationSummary:
        """ç”Ÿæˆä¼˜åŒ–æ€»ç»“"""
        # åŸºçº¿æ€§èƒ½
        baseline_perf = {}
        if baseline_analysis.get('summary'):
            baseline_perf = {
                'avg_response_time': statistics.mean([
                    metrics['avg_response_time'] 
                    for metrics in baseline_analysis['summary'].values()
                ]),
                'avg_success_rate': statistics.mean([
                    metrics['success_rate'] 
                    for metrics in baseline_analysis['summary'].values()
                ])
            }
        
        # ä¼˜åŒ–åæ€§èƒ½
        optimized_perf = {}
        improvement_metrics = {}
        
        if optimization_analysis.get('improvements'):
            improvements = list(optimization_analysis['improvements'].values())
            improvement_metrics = {
                'avg_response_time_improvement': statistics.mean([
                    imp['response_time_improvement'] for imp in improvements
                ]),
                'avg_success_rate_improvement': statistics.mean([
                    imp['success_rate_improvement'] for imp in improvements
                ])
            }
            
            # è®¡ç®—ä¼˜åŒ–åçš„ç»å¯¹æ€§èƒ½
            if baseline_perf:
                optimized_perf = {
                    'avg_response_time': baseline_perf['avg_response_time'] * (
                        1 - improvement_metrics['avg_response_time_improvement'] / 100
                    ),
                    'avg_success_rate': baseline_perf['avg_success_rate'] + 
                                      improvement_metrics['avg_success_rate_improvement']
                }
        
        # ç”Ÿæˆå»ºè®®
        recommendation = self.generate_recommendation(
            baseline_analysis, optimization_analysis, improvement_metrics
        )
        
        # è¯„ä¼°ç½®ä¿¡åº¦
        confidence_level = self.assess_confidence_level(
            baseline_analysis, optimization_analysis
        )
        
        return OptimizationSummary(
            baseline_performance=baseline_perf,
            optimized_performance=optimized_perf,
            improvement_metrics=improvement_metrics,
            recommendation=recommendation,
            confidence_level=confidence_level
        )
    
    def generate_recommendation(self, baseline_analysis: Dict, 
                              optimization_analysis: Dict, 
                              improvement_metrics: Dict) -> str:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        if not improvement_metrics:
            return "æ•°æ®ä¸è¶³ï¼Œå»ºè®®é‡æ–°æ‰§è¡Œå®Œæ•´çš„æµ‹è¯•æµç¨‹"
        
        avg_improvement = improvement_metrics.get('avg_response_time_improvement', 0)
        
        if avg_improvement > 20:
            return (
                "ğŸ‰ ä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼å»ºè®®ç«‹å³åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²RocketMQå¼‚æ­¥ä¼˜åŒ–æ–¹æ¡ˆã€‚"
                "é¢„æœŸå¯è·å¾—20%ä»¥ä¸Šçš„æ€§èƒ½æå‡ï¼Œæ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒã€‚"
            )
        elif avg_improvement > 10:
            return (
                "âœ… ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼å»ºè®®åˆ†é˜¶æ®µåœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼Œå…ˆä»ä½é£é™©æ¨¡å—å¼€å§‹ã€‚"
                "åŒæ—¶ç»§ç»­ç›‘æ§å’Œè°ƒä¼˜ï¼Œäº‰å–è·å¾—æ›´å¤§çš„æ€§èƒ½æå‡ã€‚"
            )
        elif avg_improvement > 5:
            return (
                "âš ï¸ ä¼˜åŒ–æ•ˆæœä¸€èˆ¬ã€‚å»ºè®®è¿›ä¸€æ­¥åˆ†æç“¶é¢ˆï¼Œè°ƒæ•´RocketMQé…ç½®å‚æ•°ï¼Œ"
                "æˆ–è€ƒè™‘ç»“åˆå…¶ä»–ä¼˜åŒ–æ‰‹æ®µï¼ˆå¦‚ç¼“å­˜ã€æ•°æ®åº“ä¼˜åŒ–ç­‰ï¼‰ã€‚"
            )
        else:
            return (
                "âŒ ä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾ã€‚å»ºè®®é‡æ–°è¯„ä¼°ä¼˜åŒ–æ–¹æ¡ˆï¼Œå¯èƒ½éœ€è¦ï¼š"
                "1) åˆ†ææ˜¯å¦é€‰æ‹©äº†æ­£ç¡®çš„ä¼˜åŒ–ç‚¹ï¼›"
                "2) æ£€æŸ¥RocketMQé…ç½®æ˜¯å¦åˆç†ï¼›"
                "3) è€ƒè™‘å…¶ä»–æ¶æ„ä¼˜åŒ–æ–¹æ¡ˆã€‚"
            )
    
    def assess_confidence_level(self, baseline_analysis: Dict, 
                               optimization_analysis: Dict) -> str:
        """è¯„ä¼°ç»“æœç½®ä¿¡åº¦"""
        confidence_factors = []
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if baseline_analysis.get('summary'):
            confidence_factors.append("åŸºçº¿æ•°æ®å®Œæ•´")
        
        if optimization_analysis.get('improvements'):
            confidence_factors.append("A/Bæµ‹è¯•æ•°æ®å®Œæ•´")
        
        if self.tuning_data:
            confidence_factors.append("è°ƒä¼˜æ•°æ®å¯ç”¨")
        
        # æ£€æŸ¥æµ‹è¯•è¦†ç›–åº¦
        if baseline_analysis.get('summary') and len(baseline_analysis['summary']) >= 3:
            confidence_factors.append("æµ‹è¯•è¦†ç›–åº¦å……åˆ†")
        
        # æ£€æŸ¥æ”¹è¿›ä¸€è‡´æ€§
        if optimization_analysis.get('improvements'):
            improvements = [imp['response_time_improvement'] 
                          for imp in optimization_analysis['improvements'].values()]
            if len(improvements) > 1:
                std_dev = statistics.stdev(improvements)
                if std_dev < 10:  # æ”¹è¿›æ•ˆæœæ¯”è¾ƒä¸€è‡´
                    confidence_factors.append("ç»“æœä¸€è‡´æ€§è‰¯å¥½")
        
        confidence_score = len(confidence_factors)
        
        if confidence_score >= 4:
            return "é«˜ - æµ‹è¯•æ•°æ®å……åˆ†ï¼Œç»“æœå¯ä¿¡åº¦é«˜"
        elif confidence_score >= 2:
            return "ä¸­ - æµ‹è¯•æ•°æ®åŸºæœ¬å……åˆ†ï¼Œç»“æœå…·æœ‰å‚è€ƒä»·å€¼"
        else:
            return "ä½ - æµ‹è¯•æ•°æ®ä¸è¶³ï¼Œå»ºè®®è¡¥å……æµ‹è¯•"
    
    def generate_final_report(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        print("\nç”Ÿæˆæœ€ç»ˆä¼˜åŒ–åˆ†ææŠ¥å‘Š...")
        
        # åˆ†ææ•°æ®
        baseline_analysis = self.analyze_baseline_performance()
        optimization_analysis = self.analyze_optimization_effect()
        
        # ç”Ÿæˆå›¾è¡¨
        self.generate_comprehensive_charts(baseline_analysis, optimization_analysis)
        
        # ç”Ÿæˆæ€»ç»“
        summary = self.generate_optimization_summary(baseline_analysis, optimization_analysis)
        
        # åˆ›å»ºæŠ¥å‘Šæ–‡æ¡£
        report_content = self.create_report_document(
            baseline_analysis, optimization_analysis, summary
        )
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"{self.output_dir}/optimization_analysis_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # ä¿å­˜JSONæ•°æ®
        json_data = {
            'baseline_analysis': baseline_analysis,
            'optimization_analysis': optimization_analysis,
            'summary': {
                'baseline_performance': summary.baseline_performance,
                'optimized_performance': summary.optimized_performance,
                'improvement_metrics': summary.improvement_metrics,
                'recommendation': summary.recommendation,
                'confidence_level': summary.confidence_level
            },
            'generated_at': datetime.now().isoformat()
        }
        
        json_file = f"{self.output_dir}/optimization_analysis_data.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æœ€ç»ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {self.output_dir}")
        return self.output_dir
    
    def create_report_document(self, baseline_analysis: Dict, 
                              optimization_analysis: Dict, 
                              summary: OptimizationSummary) -> str:
        """åˆ›å»ºæŠ¥å‘Šæ–‡æ¡£"""
        report = f"""
# ç¥¨åŠ¡ç³»ç»ŸRocketMQä¼˜åŒ–æ•ˆæœåˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## æ‰§è¡Œæ‘˜è¦

### ä¼˜åŒ–ç›®æ ‡
é€šè¿‡å¼•å…¥RocketMQæ¶ˆæ¯é˜Ÿåˆ—å®ç°å¼‚æ­¥å¤„ç†ï¼Œæå‡ç¥¨åŠ¡ç³»ç»Ÿçš„æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒã€‚

### ä¸»è¦å‘ç°
- **åŸºçº¿æ€§èƒ½ç­‰çº§**: {baseline_analysis.get('performance_grade', 'æœªçŸ¥')}
- **ä¼˜åŒ–æ•ˆæœç­‰çº§**: {optimization_analysis.get('overall_effect', 'æœªçŸ¥')}
- **å¹³å‡å“åº”æ—¶é—´æ”¹è¿›**: {summary.improvement_metrics.get('avg_response_time_improvement', 0):.2f}%
- **æˆåŠŸç‡æ”¹è¿›**: {summary.improvement_metrics.get('avg_success_rate_improvement', 0):.2f}%
- **ç»“æœç½®ä¿¡åº¦**: {summary.confidence_level}

### æ ¸å¿ƒå»ºè®®
{summary.recommendation}

---

## è¯¦ç»†åˆ†æ

### 1. åŸºçº¿æ€§èƒ½åˆ†æ

#### æ€§èƒ½æ¦‚å†µ
"""
        
        if baseline_analysis.get('summary'):
            report += "\n| æ“ä½œç±»å‹ | å¹³å‡å“åº”æ—¶é—´(ms) | æˆåŠŸç‡(%) | P95å“åº”æ—¶é—´(ms) | ååé‡(req/min) |\n"
            report += "|---------|-----------------|----------|----------------|----------------|\n"
            
            for op_type, metrics in baseline_analysis['summary'].items():
                report += f"| {op_type} | {metrics['avg_response_time']:.2f} | {metrics['success_rate']:.2f} | {metrics['p95_response_time']:.2f} | {metrics['throughput']:.2f} |\n"
        
        if baseline_analysis.get('bottlenecks'):
            report += "\n#### è¯†åˆ«çš„æ€§èƒ½ç“¶é¢ˆ\n\n"
            for bottleneck in baseline_analysis['bottlenecks']:
                report += f"- **{bottleneck['operation']}**: {bottleneck['issue']} (å€¼: {bottleneck['value']:.2f})\n"
        
        report += "\n### 2. RocketMQä¼˜åŒ–æ•ˆæœåˆ†æ\n\n"
        
        if optimization_analysis.get('improvements'):
            report += "#### æ€§èƒ½æ”¹è¿›å¯¹æ¯”\n\n"
            report += "| æ“ä½œç±»å‹ | å“åº”æ—¶é—´æ”¹è¿›(%) | æˆåŠŸç‡æ”¹è¿›(%) | P95æ”¹è¿›(%) |\n"
            report += "|---------|----------------|-------------|-----------|\n"
            
            for op_type, improvement in optimization_analysis['improvements'].items():
                report += f"| {op_type} | {improvement['response_time_improvement']:+.2f} | {improvement['success_rate_improvement']:+.2f} | {improvement['p95_improvement']:+.2f} |\n"
        
        if optimization_analysis.get('significant_improvements'):
            report += "\n#### æ˜¾è‘—æ”¹è¿›é¡¹\n\n"
            for improvement in optimization_analysis['significant_improvements']:
                report += f"- **{improvement['operation']}**: {improvement['type']} ({improvement['improvement']:.2f}%)\n"
        
        if optimization_analysis.get('areas_for_improvement'):
            report += "\n#### éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–çš„é¢†åŸŸ\n\n"
            for area in optimization_analysis['areas_for_improvement']:
                report += f"- **{area['operation']}**: æ”¹è¿›å¹…åº¦è¾ƒå° ({area['improvement']:.2f}%) - {area['suggestion']}\n"
        
        report += f"""

### 3. ç»¼åˆè¯„ä¼°

#### æ€§èƒ½å¯¹æ¯”æ€»ç»“
- **ä¼˜åŒ–å‰å¹³å‡å“åº”æ—¶é—´**: {summary.baseline_performance.get('avg_response_time', 0):.2f}ms
- **ä¼˜åŒ–åå¹³å‡å“åº”æ—¶é—´**: {summary.optimized_performance.get('avg_response_time', 0):.2f}ms
- **å“åº”æ—¶é—´æ”¹è¿›**: {summary.improvement_metrics.get('avg_response_time_improvement', 0):.2f}%

- **ä¼˜åŒ–å‰å¹³å‡æˆåŠŸç‡**: {summary.baseline_performance.get('avg_success_rate', 0):.2f}%
- **ä¼˜åŒ–åå¹³å‡æˆåŠŸç‡**: {summary.optimized_performance.get('avg_success_rate', 0):.2f}%
- **æˆåŠŸç‡æ”¹è¿›**: {summary.improvement_metrics.get('avg_success_rate_improvement', 0):.2f}%

#### æŠ•èµ„å›æŠ¥ç‡(ROI)åˆ†æ
- **æŠ€æœ¯æŠ•å…¥**: ä¸­ç­‰ï¼ˆRocketMQéƒ¨ç½²å’Œé…ç½®ï¼‰
- **å¼€å‘æˆæœ¬**: ä½ï¼ˆä¸»è¦æ˜¯å¼‚æ­¥å¤„ç†æ”¹é€ ï¼‰
- **æ€§èƒ½æ”¶ç›Š**: {'é«˜' if summary.improvement_metrics.get('avg_response_time_improvement', 0) > 15 else 'ä¸­ç­‰' if summary.improvement_metrics.get('avg_response_time_improvement', 0) > 5 else 'è¾ƒä½'}
- **ç»´æŠ¤æˆæœ¬**: ä½ï¼ˆRocketMQè¿ç»´ç›¸å¯¹ç®€å•ï¼‰

---

## å®æ–½å»ºè®®

### çŸ­æœŸè¡ŒåŠ¨è®¡åˆ’ï¼ˆ1-2å‘¨ï¼‰
1. **å¦‚æœä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼ˆ>15%æ”¹è¿›ï¼‰**ï¼š
   - å‡†å¤‡ç”Ÿäº§ç¯å¢ƒRocketMQéƒ¨ç½²
   - åˆ¶å®šç°åº¦å‘å¸ƒè®¡åˆ’
   - å‡†å¤‡å›æ»šæ–¹æ¡ˆ

2. **å¦‚æœä¼˜åŒ–æ•ˆæœä¸€èˆ¬ï¼ˆ5-15%æ”¹è¿›ï¼‰**ï¼š
   - è¿›ä¸€æ­¥è°ƒä¼˜RocketMQé…ç½®
   - åˆ†æå…¶ä»–æ½œåœ¨ä¼˜åŒ–ç‚¹
   - è€ƒè™‘ç»“åˆå…¶ä»–ä¼˜åŒ–æ‰‹æ®µ

3. **å¦‚æœä¼˜åŒ–æ•ˆæœä¸æ˜æ˜¾ï¼ˆ<5%æ”¹è¿›ï¼‰**ï¼š
   - é‡æ–°è¯„ä¼°ä¼˜åŒ–ç­–ç•¥
   - åˆ†ææ˜¯å¦é€‰æ‹©äº†æ­£ç¡®çš„ä¼˜åŒ–ç›®æ ‡
   - è€ƒè™‘å…¶ä»–æ¶æ„ä¼˜åŒ–æ–¹æ¡ˆ

### ä¸­æœŸä¼˜åŒ–ç­–ç•¥ï¼ˆ1ä¸ªæœˆï¼‰
- å»ºç«‹æŒç»­æ€§èƒ½ç›‘æ§ä½“ç³»
- å®æ–½åŠ¨æ€è°ƒä¼˜æœºåˆ¶
- æ‰©å±•å¼‚æ­¥å¤„ç†åˆ°æ›´å¤šä¸šåŠ¡åœºæ™¯
- ä¼˜åŒ–æ¶ˆæ¯é˜Ÿåˆ—é…ç½®å‚æ•°

### é•¿æœŸå‘å±•è§„åˆ’ï¼ˆ3ä¸ªæœˆï¼‰
- å»ºç«‹å®Œæ•´çš„æ€§èƒ½ç®¡ç†ä½“ç³»
- åˆ¶å®šæ€§èƒ½SLAæ ‡å‡†
- å®æ–½è‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•
- æŒç»­æ¶æ„ä¼˜åŒ–å’ŒæŠ€æœ¯å‡çº§

---

## é£é™©è¯„ä¼°ä¸ç¼“è§£

### æŠ€æœ¯é£é™©
- **æ¶ˆæ¯ä¸¢å¤±é£é™©**: é€šè¿‡æŒä¹…åŒ–å’Œç¡®è®¤æœºåˆ¶ç¼“è§£
- **ç³»ç»Ÿå¤æ‚åº¦å¢åŠ **: é€šè¿‡å®Œå–„ç›‘æ§å’Œæ–‡æ¡£ç¼“è§£
- **ä¾èµ–ç»„ä»¶æ•…éšœ**: é€šè¿‡é›†ç¾¤éƒ¨ç½²å’Œé™çº§æœºåˆ¶ç¼“è§£

### ä¸šåŠ¡é£é™©
- **æ€§èƒ½å›é€€é£é™©**: é€šè¿‡å……åˆ†æµ‹è¯•å’Œç°åº¦å‘å¸ƒç¼“è§£
- **æ•°æ®ä¸€è‡´æ€§é£é™©**: é€šè¿‡äº‹åŠ¡æ¶ˆæ¯å’Œè¡¥å¿æœºåˆ¶ç¼“è§£

---

## é™„å½•

### æµ‹è¯•ç¯å¢ƒä¿¡æ¯
- **æµ‹è¯•æ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}
- **æµ‹è¯•å·¥å…·**: è‡ªç ”æ€§èƒ½æµ‹è¯•æ¡†æ¶
- **æµ‹è¯•æ•°æ®**: æ¨¡æ‹ŸçœŸå®ä¸šåŠ¡åœºæ™¯
- **æµ‹è¯•æŒç»­æ—¶é—´**: åŸºçº¿æµ‹è¯• + A/Bæµ‹è¯• + æŒç»­ç›‘æ§

### ç›¸å…³æ–‡ä»¶
- æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾: `performance_radar.png`
- æ”¹è¿›æ•ˆæœæŸ±çŠ¶å›¾: `improvement_bar_chart.png`
- æ€§èƒ½è¶‹åŠ¿å›¾: `performance_trend.png`
- ç“¶é¢ˆåˆ†æå›¾: `bottleneck_analysis.png`
- è¯¦ç»†æ•°æ®: `optimization_analysis_data.json`

---

**æŠ¥å‘Šç”Ÿæˆå™¨ç‰ˆæœ¬**: 1.0  
**è”ç³»æ–¹å¼**: å¼€å‘å›¢é˜Ÿ
"""
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ç¥¨åŠ¡ç³»ç»ŸRocketMQä¼˜åŒ–æ•ˆæœåˆ†ææŠ¥å‘Šç”Ÿæˆå™¨")
    print("="*60)
    
    generator = OptimizationReportGenerator()
    
    # æ”¶é›†æµ‹è¯•æ•°æ®
    if not generator.collect_test_data():
        print("âŒ æ•°æ®æ”¶é›†å¤±è´¥ï¼Œè¯·ç¡®ä¿å·²æ‰§è¡Œç›¸å…³æµ‹è¯•")
        return
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    report_dir = generator.generate_final_report()
    
    print("\n" + "="*60)
    print("ğŸ“Š ä¼˜åŒ–åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("="*60)
    print(f"\nğŸ“ æŠ¥å‘Šä½ç½®: {report_dir}")
    print("\nğŸ“‹ æŠ¥å‘Šå†…å®¹:")
    print("  - optimization_analysis_report.md: è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("  - optimization_analysis_data.json: åŸå§‹åˆ†ææ•°æ®")
    print("  - performance_radar.png: æ€§èƒ½å¯¹æ¯”é›·è¾¾å›¾")
    print("  - improvement_bar_chart.png: æ”¹è¿›æ•ˆæœæŸ±çŠ¶å›¾")
    print("  - performance_trend.png: æ€§èƒ½è¶‹åŠ¿å›¾")
    print("  - bottleneck_analysis.png: ç“¶é¢ˆåˆ†æå›¾")
    
    print("\nğŸ¯ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. æŸ¥çœ‹è¯¦ç»†åˆ†ææŠ¥å‘Š")
    print("  2. æ ¹æ®å»ºè®®åˆ¶å®šå®æ–½è®¡åˆ’")
    print("  3. å¦‚æ•ˆæœæ˜¾è‘—ï¼Œå‡†å¤‡ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²")
    print("  4. æŒç»­ç›‘æ§å’Œä¼˜åŒ–æ€§èƒ½")
    
    # æ‰“å¼€æŠ¥å‘Šç›®å½•
    import subprocess
    try:
        subprocess.run(['explorer', report_dir], check=True)
        print(f"\nğŸ“‚ å·²æ‰“å¼€æŠ¥å‘Šç›®å½•: {report_dir}")
    except:
        print(f"\nğŸ’¡ è¯·æ‰‹åŠ¨æ‰“å¼€æŠ¥å‘Šç›®å½•æŸ¥çœ‹: {report_dir}")

if __name__ == "__main__":
    main()